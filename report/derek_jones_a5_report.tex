\documentclass{article}
\usepackage{geometry}
\geometry{left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm}
\usepackage[dvipsnames]{xcolor}

\usepackage{fancyvrb}
\usepackage{verbatim}
\usepackage{amsmath}


%\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}%
%{fontsize=\footnotesize,
 %
% frame=lines,  % top and bottom rule only
% framesep=2em, % separation between frame and text
% rulecolor=\color{Gray},
 %
% label=\fbox{\color{Black}data.txt},
% labelposition=topline,
 %
% commandchars=\|\(\), % escape character and argument delimiters for
                      % commands within the verbatim
% commentchar=*        % comment character
%}


\author{Derek Jones}
\title{CS 636: Assignment 5}

\begin{document}
\maketitle



\section*{Part 1}

The resulting decomposition of \emph{starter.py}:
\begin{itemize}
	\item \emph{train.py}: this file contains a function that takes as input a model object, a model\_name string, number of training epochs, and class weights for the loss function. Then the model is trained according to the parameters, outputing the loss, f-measure,precision, and recall statistics at each training step. Lastly, the model is saved to .h5 format for purpose of reloading the model in order to visualize and evaluate its performance, the filepath of the trained model is returned.
	\item \emph{evaluate.py}: this file contains a function that takes as input a file path to a saved .h5 model, then evaluates the model on testing data, outputting the loss,f-measure,precision, and recall statistics. Other optional arguments allow for class\_weights to be passed, and for whether or not the results should be written to a file or printed to the shell. 
	\item \emph{visualize.py}: this file, similar to \emph{evaluate.py} takes as input a file path to a saved .h5 model, and an optional class\_weights argument. Then using the provided visualization loop in \emph{starter.py}, the visualizations based upon the input paramters are shown via matplotlib.pyplot.  
	\item \emph{models.py}: this file contains the definitions of each of the models used in the experiments, including the provided default model and other models that were used to generate the results for part 3.

\end{itemize}


\section*{Part 2}

For part 2, the task was to perform a pixel level classification of background,foreground, or unknown pixels. The default model was trained using 3 sets of class weights for 50 epochs for each setting:
\begin{itemize}
	\item $[0.25,0,1]$: where predicting a true foreground pixel as background or unknown has the highest penalty, classifying a true background pixel as foreground or unknown has $\frac{1}{4}$ of the penalty, and classifying an unknown pixel results in no penalty.
	\item $[.01,0,1$: where predicting a true foreground pixel as background has the highest penalty, classifying a true background pixel as foreground or unknown has $\frac{1}{100}$ of the penalty, and classifying an unknown pixel results in no penalty. 
	\item $[1,0,1]$: where predicting a true foreground pixel as background results in the same penalty as classifying a true background pixedl as foreground or unknown, while classifying an unknown pixel results in no penalty.
	\end{itemize}

	The results of each of the class\_weights settings are given below:
	\subsection*{Default Weights}
		\verbatiminput{starter_model_p2_default_weights.txt}

	\subsection*{Reduced Background Penalty Weights}
		\verbatiminput{starter_model_p2_reduced_background_weights.txt}


	\subsection*{Balanced Penalty Weights}
		\verbatiminput{starter_model_p2_balanced_weights.txt}

In terms of Precision, it can be observed that the balanced penalty weights performed the best out of the three settings. The recall measure is greatest for the reduced background penalty weights, and the F-measure is greatest for the default weights, while only differing by a small factor ($\simeq 0.002$). Therefore the balanced penalty weights are believed to be the best choice as they are near optimal in two out of the three measures. Importantly this setting of the weights is able to achieve the best performace for classifying true positives, however it does carry the lowest score for recall meaning that it may cause an increased number of positive labels as being negative, i.e. it may be more susceptible to classifying foreground pixels as background. However due to its overall performance across all measures, this seems to be a reasonable setting for later experiments described in the next section.

\section*{Part 3}

For Part 3, I implement a new network that takes the structure of the given network in \emph{starter.py}, and adds dilations to each of the convolutional layers in order to increase the spread of the receptive field of each of the respective layers. As there are 3 convolutional layers in which dilations are introduced, the beginning dilation factor is = 2, the second = 4, and the third = 8. The motivation behind this was to gradually increase the spread of the receptive field as the input passes deeper into the network. A similar strategy was taken by the authors of WaveNet (reference?) therefore it was deemed reasonable to take this approach even though the data in this setting is not sequential, but is spatial in nature. The parameter of interest in this section was the activation function. The baseline network here uses the same Leaky ReLU activation as the given network in \emph{starter.py}. Three additional variants are introduced using distinct activations including ELU, PReLU, and ReLU. The purpose in doing this is to understand how the choice of activation may affect the results during testing time. Excluding Leaky ReLU, all activations considered use the default keras settings.

\subsection*{baseline}
The baseline network uses the Leaky ReLU activation ($\alpha = 0.1$). The evaluation results are given below:
		\verbatiminput{model_0_p3_baseline_balanced_weights.txt}
		
\subsection*{PReLU Activation}
The first modification changes the activations in the baseline network to PReLU, otherwise known as the Parameterized Rectified Linear Unit. Similar to the Leaky ReLU, PReLU allows negative values, however rather than holding the slope of the line from $[-\infty,0]$ to be constant, the slope coefficient becomes a learned parameter. As one can observe, the network is able to achieve a significantly more impressive performance in each category versus the baseline method, including loss.
		\verbatiminput{model_1_p3_prelu_balanced_weights.txt}

\subsection*{ELU Activation}
The second modification changes the activations in the baseline network to the ELU, otherwise known as the Exponential Rectified Linear Unit. This variant replaces the linear function on the interval $[-\infty,0]$ with an exponential curve that curves towards 0 as steps in the negative independent variable direction are taken. As one can observe, the results using the ELU activation (default settings) achievea higher precision score as compared to the baseline implementation, however this variant fails to outperform in both recall as well as f-measure, as well as having a higher loss than the baseline method.
		\verbatiminput{model_2_p3_elu_balanced_weights.txt}

\subsection*{ReLU Activation}
The third and final modification simply changes the activations to the ReLU activation. This activation does not allow for negative values to pass and instead maps the values on the interval $[-\infty,0]$ to 0, only allowing positive values to pass through an identity function. The results for this variant show a higher precision score as compared to the baseline method, however fails to improve on each of the other scores including loss. 
		\verbatiminput{model_3_p3_relu_balanced_weights.txt}


\section{Conclusions}
\subsection{Part 2}
After comparing the performance of each of the class weight variants, the balanced weight variant appeared to show the most promise as it outperformed each of the other methods in terms of precision, which gives a measure of how well the network is able to distinguish true positives from false positives, and performed competitively with the default weight setting with respect to the f-measure which gives a weighted average of both precision and recall. The recall measure for the balanced weight variant was not particularly impressive, which means that the network may be classifying significantly more false negatives. However as no class weight variant was optimal in every category, the balanced class weight variant was chosen for the experiments conducted in part 3.


\subsection{Part3}
In this section 4 networks were trained and compared in order to understand how changes in the activation functions affect performance during training and testing. Each network keeps the same structure, which is primarily the same as the provided model, with the addition of cascading dilation factors in the convolutional layers. A baseline method was defined to be precisely of the form just defined, with the same activation function as the provdided example network, the Leaky ReLU. Three additional variants were introduced with distinct activations. After evaluating each of the variants for 50 epochs, the model variant with the PReLU activation outperformed each of the other methods in terms of recall, f-measure, and loss, while perfoming competitively in terms of precision. The conclusion of this set of experiments suggests that the PReLU activation function, with respect to this network  architecture, is the optimal activation for the task of pixel level classification.


\section*{Appendix}
\begin{equation}
Precision: ~ P = \frac{\sum TP}{\sum TP + FP}
\end{equation}

\begin{equation}
Recall: ~ R = \frac{\sum TP}{\sum TP + FN}
\end{equation}

\begin{equation}
F-measure: ~ F1 = 2*\frac{P*R}{P + R}
\end{equation}


\end{document}